{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQ-oqtt-rs4r"
   },
   "source": [
    "# Simple Web Crawler Implementation\n",
    "\n",
    "A simple web crawler designed here is composed of 4 main modules:\n",
    "* <b>Scheduler</b>: maintain a queue of URLs to visit\n",
    "* <b>Downloader</b>: download web pages\n",
    "* <b>Analyzer</b>: analyze content and links\n",
    "* <b>Storage</b>: store content and metadata\n",
    "\n",
    "## 1) Basic Downloader\n",
    "Every web crawler should be defined a <i>name</i> and identified its <i>owner</i> (i.e., the '`user-agent`' and '`from`' fields of the headers, respectively). Sometimes, you may get an error message, caused by the connection timeout and the page not found, for instance. You can print '`response.status_code`' to track that problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "CNNj-7onrs43"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'My User Agent 1.0',\n",
    "    'From': 'your_email@domain.com'\n",
    "}\n",
    "seed_url = 'https://www.ku.ac.th/th/'\n",
    "\n",
    "def get_page(url):\n",
    "    global headers\n",
    "    text = ''\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=2)\n",
    "        # If the response was successful, no Exception will be raised\n",
    "        response.raise_for_status()\n",
    "    except HTTPError as http_err:\n",
    "        print(f'HTTP error occurred: {http_err}')  # Python 3.6\n",
    "    except Exception as err:\n",
    "        print(f'Other error occurred: {err}')  # Python 3.6\n",
    "    else:\n",
    "        print('Success!')\n",
    "        text = response.text\n",
    "    return text.lower()\n",
    "\n",
    "raw_html = get_page(seed_url)\n",
    "print(raw_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRLzU_v-rs45"
   },
   "source": [
    "## 2) Basic Analyzer\n",
    "### 2.1 Link Parser\n",
    "The following code is an example of simple link parser. The program extracts all links by considering the <i>anchor</i> tag only, and stores them into a `urls` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "y1gbwch2rs46"
   },
   "outputs": [],
   "source": [
    "def link_parser(raw_html):\n",
    "    urls = [];\n",
    "    pattern_start = '<a href=\"';  pattern_end = '\"'\n",
    "    index = 0;  length = len(raw_html)\n",
    "    while index < length:\n",
    "        start = raw_html.find(pattern_start, index)\n",
    "        if start > 0:\n",
    "            start = start + len(pattern_start)\n",
    "            end = raw_html.find(pattern_end, start)\n",
    "            link = raw_html[start:end]\n",
    "            if len(link) > 0:\n",
    "                if link not in urls:\n",
    "                    urls.append(link)\n",
    "            index = end\n",
    "        else:\n",
    "            break\n",
    "    return urls\n",
    "\n",
    "raw_html = '<html><body><a href=\"http://test1.com\">test1</a><br><a href=\"http://test2.com\">test2</a></body></html>'\n",
    "print(link_parser(raw_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHQgetnhrs47"
   },
   "source": [
    "### 2.2 URL Normalization\n",
    "The following code is an example of using the `urljoin()` function to transform a relative URL to the absolute one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "AUd5NXPirs48"
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "\n",
    "# Define an absolute (base) URL of a web page\n",
    "base_url = 'https://mike.cpe.ku.ac.th'\n",
    "\n",
    "# An example of the extracted absolute link\n",
    "link_1 = 'http://www.ku.ac.th'\n",
    "# An example of the extracted relative link\n",
    "link_2 = 'download/homework.html'\n",
    "\n",
    "# Resolve links\n",
    "abs_link_1 = urljoin(base_url, link_1)\n",
    "abs_link_2 = urljoin(base_url, link_2)\n",
    "\n",
    "print(abs_link_1)  # -> http://www.ku.ac.th\n",
    "print(abs_link_2)  # -> https://mike.cpe.ku.ac.th/download/homework.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "hJZTdKp4rs49"
   },
   "source": [
    "## 3) Basic Scheduler\n",
    "The following code is an example of using a FIFO queue to handle the extracted URLs to be further downloaded. In particular, the main crawling process simply invokes the previous two defined functions, i.e., `get_page()` and `link_parser()`, to first download a web page and extract its out-going links, respectively. Then, all extracted links will be stored into a queue. We define here two queues: `frontier_q` and `visited_q`. The former is used as the FIFO queue to keep URLs for next downloading, while the latter is used to remember which web pages have been already downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Dhe0_Rtjrs49",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed_url = 'https://www.ku.ac.th/th/'\n",
    "frontier_q = [seed_url]\n",
    "visited_q = []\n",
    "\n",
    "# @param 'links' is a list of extracted links to be stored in the queue\n",
    "def enqueue(links):\n",
    "    global frontier_q\n",
    "    for link in links:\n",
    "        if link not in frontier_q and link not in visited_q:\n",
    "            frontier_q.append(link)\n",
    "\n",
    "# FIFO queue\n",
    "def dequeue():\n",
    "    global frontier_q\n",
    "    current_url = frontier_q[0]\n",
    "    frontier_q = frontier_q[1:]\n",
    "    return current_url\n",
    "\n",
    "#--- main process ---#\n",
    "current_url = dequeue()\n",
    "visited_q.append(current_url)\n",
    "raw_html = get_page(current_url)\n",
    "extracted_links = link_parser(raw_html)\n",
    "enqueue(extracted_links)\n",
    "print(frontier_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Flt5hpvars4-"
   },
   "source": [
    "## 4) Storing Text into a File\n",
    "As the following, we use the `os.makedirs()` function to first create (sub)directories. Notice that the `exist_ok=True` parameter is set to prevent an exception error if the target directory already exists. Then, we use the `open()`, `write()`, and `close()` functions to open a file, write some text into that file, and afterwards close it. In addition, we import the `codecs` module together with using the '`utf-8`' encoding for non-English content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "UXxunyt9rs4_"
   },
   "outputs": [],
   "source": [
    "import os, codecs\n",
    "\n",
    "# Create (sub)directories with the 0o755 permission\n",
    "# @param 'exist_ok' is True for no exception if the target directory already exists\n",
    "path = 'html/subdir1/subdir2'\n",
    "os.makedirs(path, 0o755, exist_ok=True)\n",
    "\n",
    "# Write content into a file\n",
    "raw_html = '<html><body><a href=\"http://test1.com\">test1</a><br><a href=\"http://test2.com\">test2</a></body></html>'\n",
    "abs_file = path + '/index.html'\n",
    "f = codecs.open(abs_file, 'w', 'utf-8')\n",
    "f.write(raw_html)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UDEMrnxrs4_"
   },
   "source": [
    "# <font color=\"blue\">Your Turn ...</font>\n",
    "Write a web crawler to collect 10,000 web pages (including only '`.htm`' and '`.html`' files) within the '`ku.ac.th`' domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "PV2vQc_Trs5A"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "simple_web_crawler.ipynb",
   "provenance": [
    {
     "file_id": "1w71jQERMM1P1NiMz13v3M4wwY72oy6L5",
     "timestamp": 1608560605021
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
